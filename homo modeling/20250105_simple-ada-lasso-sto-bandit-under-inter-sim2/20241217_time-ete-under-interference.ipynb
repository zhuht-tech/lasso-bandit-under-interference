{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9902420-c692-4b92-8f14-cc9e38f5ad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.spatial import distance\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "import numpy as np\n",
    "from scipy.optimize import differential_evolution\n",
    "from scipy.stats import rankdata\n",
    "from sklearn.cross_decomposition import CCA\n",
    "import math\n",
    "import copy\n",
    "from math import log\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from sklearn.linear_model import Lasso\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "066e083a-5463-48c4-85ff-9f72c3f34c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: [ 3.87593318  0.12303241 -0.82319167  2.8958156   4.70534406  0.4141024\n",
      "  3.33198974  5.11773944  5.36558892  2.8544744 ]\n"
     ]
    }
   ],
   "source": [
    "with open('relation_matrix_dic.pkl', 'rb') as f:\n",
    "    relation_matrix_dic = pickle.load(f)\n",
    "\n",
    "# define a new adjacency_matrix_dic: adjacency_matrix_dic[i] = relation_matrix_dic[i].iloc[0:20, 0:20]\n",
    "adjacency_matrix_dic = {i: relation_matrix_dic[i].iloc[0:20, 0:20] for i in relation_matrix_dic}\n",
    "# Read beta back from the file\n",
    "beta = np.load('beta.npy')\n",
    "print(\"beta:\", beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d332fd2c-fa0a-423e-85b3-3696d82b78e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(adjacency_matrix_dic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdf4173d-6c1e-48b5-8a95-c4c812402b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# follow the beta, only 1,2,3 related is non-zero\n",
    "K = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "122ff145-5cc5-4027-884e-8ca6906e5242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_beta: [ 3.87593318  0.12303241 -0.82319167  2.8958156   0.          0.\n",
      "  0.          0.          0.          0.          4.70534406  0.4141024\n",
      "  3.33198974  0.          0.          0.          0.          0.\n",
      "  0.          5.11773944  5.36558892  0.          0.          0.\n",
      "  0.          0.          0.          2.8544744   0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.        ]\n"
     ]
    }
   ],
   "source": [
    "# change beta into length 1 + 9 + 9 + choose 2 from 9\n",
    "# with initial beta all 0\n",
    "# beta[0:4] = beta[0:4] , i.e. 0,1,2,3\n",
    "# beta[11:14] = beta[4:7], i.e. (1,1), (1,2), (1,3)\n",
    "# beta[20 and 21] = beta[7 and 8]\n",
    "# beta [28] = beta[9]\n",
    "\n",
    "# Initialize a new beta array of length 55\n",
    "new_beta = np.zeros(55)\n",
    "\n",
    "# Assign values based on the provided instructions\n",
    "new_beta[0:4] = beta[0:4]            # Copy the first four elements unchanged\n",
    "new_beta[10:13] = beta[4:7]          # Copy beta[4:6] into new_beta[11:13]\n",
    "new_beta[19:21] = beta[7:9]          # Copy beta[7:8] into new_beta[20:21]\n",
    "new_beta[27] = beta[9]               # Copy beta[9] into new_beta[28]\n",
    "\n",
    "# Print the new_beta array\n",
    "print(\"new_beta:\", new_beta)\n",
    "\n",
    "beta = new_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "246a72a4-4af8-400a-8999-31402191f36e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position 0: beta = 3.8759331788989613\n",
      "Position 1: beta = 0.12303240864427623\n",
      "Position 2: beta = -0.8231916699248809\n",
      "Position 3: beta = 2.895815598774753\n",
      "Position 4: beta = 0.0\n",
      "Position 5: beta = 0.0\n",
      "Position 6: beta = 0.0\n",
      "Position 7: beta = 0.0\n",
      "Position 8: beta = 0.0\n",
      "Position 9: beta = 0.0\n",
      "Position (1,1): beta = 4.7053440610515\n",
      "Position (1,2): beta = 0.4141024027012443\n",
      "Position (1,3): beta = 3.331989738972724\n",
      "Position (1,4): beta = 0.0\n",
      "Position (1,5): beta = 0.0\n",
      "Position (1,6): beta = 0.0\n",
      "Position (1,7): beta = 0.0\n",
      "Position (1,8): beta = 0.0\n",
      "Position (1,9): beta = 0.0\n",
      "Position (2,2): beta = 5.117739436696092\n",
      "Position (2,3): beta = 5.365588921449674\n",
      "Position (2,4): beta = 0.0\n",
      "Position (2,5): beta = 0.0\n",
      "Position (2,6): beta = 0.0\n",
      "Position (2,7): beta = 0.0\n",
      "Position (2,8): beta = 0.0\n",
      "Position (2,9): beta = 0.0\n",
      "Position (3,3): beta = 2.854474397693938\n",
      "Position (3,4): beta = 0.0\n",
      "Position (3,5): beta = 0.0\n",
      "Position (3,6): beta = 0.0\n",
      "Position (3,7): beta = 0.0\n",
      "Position (3,8): beta = 0.0\n",
      "Position (3,9): beta = 0.0\n",
      "Position (4,4): beta = 0.0\n",
      "Position (4,5): beta = 0.0\n",
      "Position (4,6): beta = 0.0\n",
      "Position (4,7): beta = 0.0\n",
      "Position (4,8): beta = 0.0\n",
      "Position (4,9): beta = 0.0\n",
      "Position (5,5): beta = 0.0\n",
      "Position (5,6): beta = 0.0\n",
      "Position (5,7): beta = 0.0\n",
      "Position (5,8): beta = 0.0\n",
      "Position (5,9): beta = 0.0\n",
      "Position (6,6): beta = 0.0\n",
      "Position (6,7): beta = 0.0\n",
      "Position (6,8): beta = 0.0\n",
      "Position (6,9): beta = 0.0\n",
      "Position (7,7): beta = 0.0\n",
      "Position (7,8): beta = 0.0\n",
      "Position (7,9): beta = 0.0\n",
      "Position (8,8): beta = 0.0\n",
      "Position (8,9): beta = 0.0\n",
      "Position (9,9): beta = 0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"Position {i}: beta = {beta[i]}\")\n",
    "\n",
    "idx = 10\n",
    "\n",
    "for i in range(1, 10):\n",
    "    for j in range(1, 10):\n",
    "        if i <= j:\n",
    "            print(f\"Position ({i},{j}): beta = {beta[idx]}\")\n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83e845e5-ddca-4ecc-b0c2-d73301f07fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function named \"counting_treatment\"\n",
    "def counting_treatment(adjacency_matrix, A, K):\n",
    "\n",
    "    # adjacency_matrix is a adjacency matrix dataframe with length 20 * 20 known\n",
    "    # A is a np.array with length 20, each element 1,2,...,K known corresponding to each node in the network\n",
    "\n",
    "    # a empty df named counting_treatment with column named: 0,1,...,K, (1,1), ..., (1,K), (2,2), ..., (2,K),...(K,K)\n",
    "\n",
    "    # add different numbers of different nodes and different edges. For the column 0, directly give 1.\n",
    "\n",
    "    # Define column names\n",
    "    basic_columns = list(range(0, K + 1))  # Including column 0\n",
    "    edge_columns = [(i, j) for i in range(1, K + 1) for j in range(i, K + 1)]\n",
    "    columns = basic_columns + edge_columns\n",
    "    \n",
    "    # Create an empty DataFrame\n",
    "    counting_treatment_df = pd.DataFrame(columns=columns)\n",
    "    counting_treatment_df.loc[0] = 0  # Initialize all counts to 0\n",
    "    \n",
    "    # Column 0 gets the value of 1 as per the description\n",
    "    counting_treatment_df.at[0, 0] = 1\n",
    "\n",
    "    # Count number of nodes with each treatment\n",
    "    for treatment in range(1, K + 1):\n",
    "        counting_treatment_df.at[0, treatment] = sum(A == treatment)\n",
    "    \n",
    "    # Count the number of edges connecting nodes of different treatments\n",
    "    for i in range(len(adjacency_matrix_dic[0])):\n",
    "        for j in range(i + 1, len(adjacency_matrix_dic[0])):  # Iterate only for j > i to avoid double-counting\n",
    "            if adjacency_matrix.iloc[i, j] != 0:  # If there's an edge between node i and node j\n",
    "                treatment_i = A[i]\n",
    "                treatment_j = A[j]\n",
    "                # Use sorted tuple to represent undirected edge\n",
    "                edge = tuple(sorted((treatment_i, treatment_j)))\n",
    "                if edge in counting_treatment_df.columns:\n",
    "                    counting_treatment_df.at[0, edge] += 1\n",
    "                \n",
    "    return counting_treatment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9820eed2-924b-499f-9091-60571f5e32bf",
   "metadata": {},
   "source": [
    "# generate_outcome "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "095cf8e3-4b66-4d19-8554-349b2635b530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_outcome(adjacency_matrix, A, K, beta, error):\n",
    "    # Generate the treatment matrix using the counting_treatment function\n",
    "    counting_treatment_df = counting_treatment(adjacency_matrix, A, K)\n",
    "\n",
    "    # Convert the treatment matrix to a NumPy array for easier calculation\n",
    "    counting_treatment_array = counting_treatment_df.values.flatten()\n",
    "\n",
    "    # Ensure that beta and counting_treatment_array have the same length\n",
    "    if len(beta) != len(counting_treatment_array):\n",
    "        raise ValueError(\"Length of beta does not match the number of columns in counting_treatment.\")\n",
    "\n",
    "    # Compute the outcome Y\n",
    "    Y_exp = np.dot(counting_treatment_array, beta)\n",
    "    \n",
    "    Y = np.dot(counting_treatment_array, beta) + error\n",
    "\n",
    "    # Return the results\n",
    "    return Y, Y_exp, counting_treatment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaa410b-ce4e-44a5-9392-2d3ef9552ad7",
   "metadata": {},
   "source": [
    "# lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa8d339e-f80c-4d8f-a141-553d0b6d850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lasso_model(experimented_data, lambda_value):\n",
    "    # Step 1: Prepare Data (X, Y)\n",
    "    # X consists of all data except 'Y' from experimented_data\n",
    "    X = experimented_data.iloc[:, :-1].values  # Take all columns except the last one (assumed 'Y' is the last column)\n",
    "    Y = experimented_data['Y'].values.reshape(-1, 1)  # Rewards (Y) as a column vector\n",
    "    \n",
    "    # Step 2: Fit Lasso Regression Model with lambda (alpha) as the regularization parameter\n",
    "    lasso_model = Lasso(alpha=lambda_value)  # Lambda corresponds to the alpha parameter in Lasso\n",
    "    \n",
    "    # Train the Lasso model\n",
    "    lasso_model.fit(X, Y.ravel())  # Use ravel to convert Y to a 1D array\n",
    "    \n",
    "    return lasso_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aae4aad6-0638-4e6a-93df-63906bd992df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lasso_reward(A, model, adjacency_matrix, K):\n",
    "\n",
    "    # Predict for new target treatments\n",
    "    # Generate new target treatments using counting_treatment function\n",
    "    counting_treatment_df = counting_treatment(adjacency_matrix, A, K)  # Get DataFrame representation\n",
    "    \n",
    "    # Convert the dataframe to a NumPy array for GPy compatibility\n",
    "    X_target = counting_treatment_df.values  # Convert the DataFrame to a NumPy array\n",
    "\n",
    "    if X_target.ndim == 1:\n",
    "        X_target = X_target.reshape(1, -1)  # Reshape to be a 2D array with one row\n",
    "\n",
    "    # Predict the rewards and the full covariance matrix for the new target treatments\n",
    "    predicted_reward = model.predict(X_target)\n",
    "\n",
    "    # Simplify the results to a scalar value if necessary\n",
    "    # For Lasso, the output of .predict() should be a 1D array, so we can just return the first value\n",
    "    \n",
    "    return predicted_reward[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad74a980-c50c-4f5d-9c05-a35965e98ec6",
   "metadata": {},
   "source": [
    "# get the parents for next generation by ucb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22a4d16f-449a-4a30-b7ab-e6f0f60179e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization genetic algorithm\n",
    "def get_children_elite(parent_treatment_dic, parent_reward_predict, model, adjacency_matrix, K, generation_num):\n",
    "    child_treatment_dic = {}\n",
    "    child_reward_predict = {}\n",
    "    \n",
    "    # Step 2: Generate children from parent treatments using roulette wheel selection\n",
    "    for i in range(generation_num):\n",
    "        # Use roulette wheel selection to choose two parents based on UCB values\n",
    "        A1, A2 = roulette_wheel_selection(parent_reward_predict, parent_treatment_dic)\n",
    "        \n",
    "        # Crossover: with 0.5 probability choose genes from A1 or A2\n",
    "        child_treatment_dic[i + generation_num] = np.where(np.random.rand(len(A1)) < 0.5, A1, A2)\n",
    "        \n",
    "        # Predict reward and UCB for the new child\n",
    "        predicted_rewards = predict_lasso_reward(child_treatment_dic[i + generation_num], model, adjacency_matrix, K)\n",
    "        child_reward_predict[i + generation_num] = predicted_rewards\n",
    "\n",
    "    # Step 3: Select the top treatments from both parents and children\n",
    "    combined_predict = {**parent_reward_predict, **child_reward_predict}\n",
    "    \n",
    "    # Sort the combined dictionary by UCB values and select the top N treatments\n",
    "    sorted_indices = sorted(combined_predict, key=combined_predict.get, reverse=True)[:generation_num]\n",
    "    child_elite = {idx: combined_predict[idx] for idx in sorted_indices}\n",
    "    \n",
    "    child_elite_treatment_dic = {}\n",
    "    for idx in sorted_indices:\n",
    "        if idx in parent_reward_predict:\n",
    "            child_elite_treatment_dic[idx] = parent_treatment_dic[idx]\n",
    "        else:\n",
    "            child_elite_treatment_dic[idx] = child_treatment_dic[idx]\n",
    "\n",
    "    return child_elite_treatment_dic, child_elite\n",
    "\n",
    "\n",
    "def roulette_wheel_selection(reward_predict, treatment_dic):\n",
    "\n",
    "    values = np.array(list(reward_predict.values()))\n",
    "\n",
    "    # Ensure no negative values by shifting if necessary\n",
    "    if np.min(values) < 0:\n",
    "        values = values - np.min(values)  # Shift values to make them non-negative\n",
    "    \n",
    "    total = np.sum(values)\n",
    "    \n",
    "    if total > 0:\n",
    "        probabilities = values / total  # Normalize to create valid probabilities\n",
    "    else:\n",
    "        # If all UCB values are zero, use uniform probability\n",
    "        probabilities = np.ones_like(values) / len(values)  # Equal probability if all UCB are zero\n",
    "\n",
    "    # Select two parents based on the computed probabilities\n",
    "    selected_indices = np.random.choice(len(values), size=2, p=probabilities, replace=False)\n",
    "    return treatment_dic[selected_indices[0]], treatment_dic[selected_indices[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8dde1c17-7766-411a-9de3-e193e878cc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the next experiment\n",
    "\n",
    "def get_next_experiment(model, adjacency_matrix, K, generation_num, epoch):\n",
    "    parent_treatment_dic = {i: np.random.randint(1, K+1, size=adjacency_matrix.shape[0]) for i in range(100)}\n",
    "    parent_reward_predict = {}\n",
    "    \n",
    "    # Step 1: Calculate UCB for parent treatments\n",
    "    for i in range(generation_num):\n",
    "        predicted_rewards = predict_lasso_reward(parent_treatment_dic[i], model, adjacency_matrix, K)\n",
    "        # UCB\n",
    "        parent_reward_predict[i] = predicted_rewards\n",
    "\n",
    "        \n",
    "    # Epoch loop\n",
    "    # Initialize variables\n",
    "    no_progress_counter = 0  # Counter to track epochs without improvement\n",
    "    best = -np.inf  # To store the best UCB value so far\n",
    "    \n",
    "    \n",
    "    # Epoch loop\n",
    "    for k in range(1, epoch + 1):\n",
    "        # Get the elite children treatments and UCBs\n",
    "        child_elite_treatment_dic, child_elite = get_children_elite(parent_treatment_dic, parent_reward_predict, model, adjacency_matrix, K, generation_num)\n",
    "        \n",
    "        # Reassign parent treatments and UCB predictions with renumbered indices\n",
    "        parent_treatment_dic = {new_idx: treatment for new_idx, treatment in enumerate(child_elite_treatment_dic.values())}\n",
    "        parent_reward_predict = {new_idx: sur for new_idx, sur in enumerate(child_elite.values())}\n",
    "    \n",
    "        # Get the best UCB in this epoch\n",
    "        current_best = parent_reward_predict[0]\n",
    "    \n",
    "        # Print the UCB of the best treatment after each epoch\n",
    "        # print(f\"Epoch {k}: Best UCB = {current_best}\")\n",
    "    \n",
    "        # Check if the best UCB has improved\n",
    "        if current_best > best:\n",
    "            best = current_best  # Update the best UCB\n",
    "            no_progress_counter = 0  # Reset the counter\n",
    "        else:\n",
    "            no_progress_counter += 1  # No improvement, increment the counter\n",
    "    \n",
    "        # If no improvement for 10 epochs, break the loop\n",
    "        if no_progress_counter >= 10:\n",
    "            # print(f\"No improvement for 10 consecutive epochs. Stopping at epoch {k}.\")\n",
    "            break\n",
    "\n",
    "    return parent_treatment_dic[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4aef02-bccc-4efc-9e9e-fda44d9f3dad",
   "metadata": {},
   "source": [
    "# initial experiment 0: for a selected vector treatments, we get rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e66a9645-0133-43bc-9c3b-1516eee3c79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploration_then_exploitation(E):\n",
    "\n",
    "    d = len(beta)\n",
    "    sigma = 1\n",
    "    lambda_value = 2 * sigma\n",
    "\n",
    "    # Initialize empty lists to store A and reward over time\n",
    "    A_time = []\n",
    "    reward_time = []\n",
    "    \n",
    "    # Create an empty DataFrame for experimented data\n",
    "    experimented_data = pd.DataFrame()\n",
    "\n",
    "    # Get the length of relation_matrix_dic (number of keys)\n",
    "    length = len(relation_matrix_dic)\n",
    "    \n",
    "    # Generate the error array from a normal distribution with the specified length\n",
    "    mean = 0  # Define the mean\n",
    "    std_dev = sigma\n",
    "    \n",
    "    error_array = mean + std_dev * np.random.randn(length)\n",
    "    \n",
    "    for t in range(E):\n",
    "        # Create an array A , with length matching the adjacency matrix\n",
    "        A = np.random.randint(1, K + 1, size=adjacency_matrix_dic[t].shape[0])\n",
    "\n",
    "        Y, Y_exp, counting_treatment_df = generate_outcome(adjacency_matrix_dic[t], A, K, beta, error_array[t])\n",
    "        \n",
    "        # Create a DataFrame for Y\n",
    "        Y_df = pd.DataFrame([Y], columns=['Y'])\n",
    "        \n",
    "        # Concatenate treatment and Y_df to form experimented_data (append vertically)\n",
    "        experiment_data_current = pd.concat([counting_treatment_df, Y_df], axis=1)\n",
    "        \n",
    "        # Append to the existing DataFrame (vertically concatenating)\n",
    "        experimented_data = pd.concat([experimented_data, experiment_data_current], axis=0, ignore_index=True)\n",
    "        \n",
    "        # Append A and total reward to the respective lists\n",
    "        reward_time.append(Y_exp)\n",
    "\n",
    "    generation_num = 100\n",
    "    epoch = 100\n",
    "\n",
    "    T = 100\n",
    "\n",
    "    model = predict_lasso_model(experimented_data, lambda_value)\n",
    "        \n",
    "    # Get the next treatment vector A\n",
    "    A = get_next_experiment(model, adjacency_matrix_dic[t], K, generation_num, epoch)\n",
    "    \n",
    "    # Generate the outcome based on A\n",
    "    Y, Y_exp, counting_treatment_df = generate_outcome(adjacency_matrix_dic[t], A, K, beta, error_array[t])\n",
    "\n",
    "    for t in range(E,T):  # Iterate for T time steps\n",
    "\n",
    "        # Get the next treatment vector A\n",
    "        A = get_next_experiment(model, adjacency_matrix_dic[t], K, generation_num, epoch)\n",
    "        \n",
    "        # Generate the outcome based on A\n",
    "        Y, Y_exp, counting_treatment_df = generate_outcome(adjacency_matrix_dic[t], A, K, beta, error_array[t])\n",
    "\n",
    "        \n",
    "        reward_time.append(Y_exp)\n",
    "    \n",
    "        # Print the reward for the current time step\n",
    "        print(f\"Time {t} with reward: {Y_exp}\")\n",
    "        # print(f\"Time {t} with treatment: {A}\")\n",
    "\n",
    "    # Convert reward_time to a NumPy array\n",
    "    reward_time = np.array(reward_time)\n",
    "\n",
    "    return reward_time\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0e4c1105-8d1f-4961-b153-20838e3bdcb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment 1 for ete\n",
      "Time 50 with reward: 507.7746431839173\n",
      "Time 51 with reward: 491.7862724389564\n",
      "Time 52 with reward: 627.1921225936421\n",
      "Time 53 with reward: 533.3378663398374\n",
      "Time 54 with reward: 574.4066469014148\n",
      "Time 55 with reward: 540.184671518545\n",
      "Time 56 with reward: 620.9548551525272\n",
      "Time 57 with reward: 461.2832636391893\n",
      "Time 58 with reward: 418.526560223781\n",
      "Time 59 with reward: 686.5630226101072\n",
      "Time 60 with reward: 572.206453930694\n",
      "Time 61 with reward: 650.8853049975398\n",
      "Time 62 with reward: 511.62069687594226\n",
      "Time 63 with reward: 420.951357998911\n",
      "Time 64 with reward: 516.9547497632512\n",
      "Time 65 with reward: 636.0917600120026\n",
      "Time 66 with reward: 508.68373838898697\n",
      "Time 67 with reward: 551.8188825599336\n",
      "Time 68 with reward: 532.7781930152864\n",
      "Time 69 with reward: 747.072862651971\n",
      "Time 70 with reward: 487.4376961538012\n",
      "Time 71 with reward: 606.3838669795\n",
      "Time 72 with reward: 712.2727042103326\n",
      "Time 73 with reward: 540.1531354844043\n",
      "Time 74 with reward: 513.6092290717188\n",
      "Time 75 with reward: 578.7423375563666\n",
      "Time 76 with reward: 627.9611450754633\n",
      "Time 77 with reward: 661.1908202325562\n",
      "Time 78 with reward: 597.324744816911\n",
      "Time 79 with reward: 642.7607520675808\n",
      "Time 80 with reward: 548.4302088652489\n",
      "Time 81 with reward: 387.4867502519231\n",
      "Time 82 with reward: 457.3105262344724\n",
      "Time 83 with reward: 444.90639042450005\n",
      "Time 84 with reward: 515.8038670800083\n",
      "Time 85 with reward: 438.02766954343497\n",
      "Time 86 with reward: 509.3004217752391\n",
      "Time 87 with reward: 596.8290458474038\n",
      "Time 88 with reward: 573.6500721472308\n",
      "Time 89 with reward: 504.2721307211473\n",
      "Time 90 with reward: 576.026564427937\n",
      "Time 91 with reward: 471.92560996585473\n",
      "Time 92 with reward: 486.064438013365\n",
      "Time 93 with reward: 512.6445331358598\n",
      "Time 94 with reward: 593.9963583246594\n",
      "Time 95 with reward: 473.8358918771401\n",
      "Time 96 with reward: 607.0260243933124\n",
      "Time 97 with reward: 438.6697398497555\n",
      "Time 98 with reward: 536.0652943142061\n",
      "Time 99 with reward: 614.5025392581953\n",
      "Running experiment 2 for ete\n",
      "Time 50 with reward: 494.0111778201948\n",
      "Time 51 with reward: 488.5027557361946\n",
      "Time 52 with reward: 622.3537686758403\n",
      "Time 53 with reward: 536.8090241237835\n",
      "Time 54 with reward: 597.3756928720317\n",
      "Time 55 with reward: 521.1666151278608\n",
      "Time 56 with reward: 588.1683620639295\n",
      "Time 57 with reward: 450.4518388255468\n",
      "Time 58 with reward: 430.2275864938108\n",
      "Time 59 with reward: 716.3858380527745\n",
      "Time 60 with reward: 600.8213766284174\n",
      "Time 61 with reward: 647.8539197634791\n",
      "Time 62 with reward: 527.3110316980717\n",
      "Time 63 with reward: 423.027616274225\n",
      "Time 64 with reward: 502.19422575185615\n",
      "Time 65 with reward: 631.8130794187518\n",
      "Time 66 with reward: 530.1970421299063\n",
      "Time 67 with reward: 554.5138722133989\n",
      "Time 68 with reward: 541.0237303619904\n",
      "Time 69 with reward: 755.94204767827\n",
      "Time 70 with reward: 511.6146348693619\n",
      "Time 71 with reward: 608.9589558582996\n",
      "Time 72 with reward: 729.9144802316666\n",
      "Time 73 with reward: 525.9253258299985\n",
      "Time 74 with reward: 545.24540089354\n",
      "Time 75 with reward: 586.9878749030705\n",
      "Time 76 with reward: 618.5962610796571\n",
      "Time 77 with reward: 681.8720294233732\n",
      "Time 78 with reward: 600.3002036313499\n",
      "Time 79 with reward: 639.6014181234323\n",
      "Time 80 with reward: 538.4810798040938\n",
      "Time 81 with reward: 368.2121398092081\n",
      "Time 82 with reward: 466.0517625506834\n",
      "Time 83 with reward: 463.41966361018274\n",
      "Time 84 with reward: 520.8900704825638\n",
      "Time 85 with reward: 442.4432017269511\n",
      "Time 86 with reward: 527.6543915720099\n",
      "Time 87 with reward: 590.7827991846581\n",
      "Time 88 with reward: 563.0772954064807\n",
      "Time 89 with reward: 510.7501019983562\n",
      "Time 90 with reward: 588.6599305833809\n",
      "Time 91 with reward: 490.2476082174312\n",
      "Time 92 with reward: 497.0689393685783\n",
      "Time 93 with reward: 511.1574362273384\n",
      "Time 94 with reward: 631.9280018289164\n",
      "Time 95 with reward: 506.93558434047196\n",
      "Time 96 with reward: 592.0546662846459\n",
      "Time 97 with reward: 440.84445879291593\n",
      "Time 98 with reward: 536.12926866925\n",
      "Time 99 with reward: 615.8697353920511\n",
      "Running experiment 3 for ete\n",
      "Time 50 with reward: 487.6531073176515\n",
      "Time 51 with reward: 489.17282390239296\n",
      "Time 52 with reward: 614.8204251045731\n",
      "Time 53 with reward: 536.4332259289421\n",
      "Time 54 with reward: 573.4398207031983\n",
      "Time 55 with reward: 511.2898965262185\n",
      "Time 56 with reward: 626.6892779754755\n",
      "Time 57 with reward: 445.1285845263767\n",
      "Time 58 with reward: 408.33848455805\n",
      "Time 59 with reward: 704.7576890179662\n",
      "Time 60 with reward: 574.4697189696963\n",
      "Time 61 with reward: 648.374190473784\n",
      "Time 62 with reward: 507.9656639622866\n",
      "Time 63 with reward: 422.10909557043215\n",
      "Time 64 with reward: 506.2934269266595\n",
      "Time 65 with reward: 615.3997522330469\n",
      "Time 66 with reward: 526.2301853764532\n",
      "Time 67 with reward: 530.8898238843631\n",
      "Time 68 with reward: 527.2276453220478\n",
      "Time 69 with reward: 731.166033023912\n",
      "Time 70 with reward: 481.5251381826872\n",
      "Time 71 with reward: 627.0896479111146\n",
      "Time 72 with reward: 704.6427666078015\n",
      "Time 73 with reward: 511.1624145918395\n",
      "Time 74 with reward: 536.6880397070386\n",
      "Time 75 with reward: 549.8155910188457\n",
      "Time 76 with reward: 600.5541151226838\n",
      "Time 77 with reward: 661.4386697173098\n",
      "Time 78 with reward: 591.4949929600948\n",
      "Time 79 with reward: 640.7453365133322\n",
      "Time 80 with reward: 512.7776415657655\n",
      "Time 81 with reward: 373.4754354034469\n",
      "Time 82 with reward: 463.1008754770424\n",
      "Time 83 with reward: 446.9531606575727\n",
      "Time 84 with reward: 500.768534616298\n",
      "Time 85 with reward: 451.2954359376504\n",
      "Time 86 with reward: 515.2826940829409\n",
      "Time 87 with reward: 584.3933740032909\n",
      "Time 88 with reward: 542.7718844105053\n",
      "Time 89 with reward: 503.40063355679865\n",
      "Time 90 with reward: 561.9235718496407\n",
      "Time 91 with reward: 473.1968601994722\n",
      "Time 92 with reward: 479.77034186586576\n",
      "Time 93 with reward: 516.2995660495155\n",
      "Time 94 with reward: 608.2399791448366\n",
      "Time 95 with reward: 492.3006218124008\n",
      "Time 96 with reward: 588.5835085006998\n",
      "Time 97 with reward: 429.71200872761483\n",
      "Time 98 with reward: 519.9106152013935\n",
      "Time 99 with reward: 606.7527008809984\n",
      "Running experiment 4 for ete\n",
      "Time 50 with reward: 499.9294757728526\n",
      "Time 51 with reward: 485.7016661184469\n",
      "Time 52 with reward: 609.7251787099652\n",
      "Time 53 with reward: 532.7142186602424\n",
      "Time 54 with reward: 573.0394507675591\n",
      "Time 55 with reward: 524.6849947641514\n",
      "Time 56 with reward: 561.2540288584128\n",
      "Time 57 with reward: 464.90694187402096\n",
      "Time 58 with reward: 421.67022530730935\n",
      "Time 59 with reward: 677.7578119388521\n",
      "Time 60 with reward: 596.4787216801227\n",
      "Time 61 with reward: 603.915277105325\n",
      "Time 62 with reward: 523.7445448802578\n",
      "Time 63 with reward: 429.7064134967433\n",
      "Time 64 with reward: 500.46367506984336\n",
      "Time 65 with reward: 628.2465926009379\n",
      "Time 66 with reward: 529.7653175154431\n",
      "Time 67 with reward: 541.2461058191835\n",
      "Time 68 with reward: 522.0311479988666\n",
      "Time 69 with reward: 744.2499242884178\n",
      "Time 70 with reward: 508.14347708541584\n",
      "Time 71 with reward: 625.13820671191\n",
      "Time 72 with reward: 718.7820301663655\n",
      "Time 73 with reward: 532.8036670428467\n",
      "Time 74 with reward: 546.0774954436424\n",
      "Time 75 with reward: 585.6520334480388\n",
      "Time 76 with reward: 595.1046548160948\n",
      "Time 77 with reward: 676.7218516657738\n",
      "Time 78 with reward: 597.324744816911\n",
      "Time 79 with reward: 630.9487279030628\n",
      "Time 80 with reward: 524.5852700190211\n",
      "Time 81 with reward: 361.9449878649633\n",
      "Time 82 with reward: 454.3596391608313\n",
      "Time 83 with reward: 454.86230242368134\n",
      "Time 84 with reward: 398.627501913625\n",
      "Time 85 with reward: 442.93890069645835\n",
      "Time 86 with reward: 524.023930399152\n",
      "Time 87 with reward: 583.185481258347\n",
      "Time 88 with reward: 562.33374695222\n",
      "Time 89 with reward: 499.1219529635479\n",
      "Time 90 with reward: 548.4168588508493\n",
      "Time 91 with reward: 474.4047529444161\n",
      "Time 92 with reward: 475.24381178786143\n",
      "Time 93 with reward: 505.1111895645927\n",
      "Time 94 with reward: 601.3616379319885\n",
      "Time 95 with reward: 503.61694700741157\n",
      "Time 96 with reward: 592.7028857050387\n",
      "Time 97 with reward: 443.0598851132032\n",
      "Time 98 with reward: 538.289156738962\n",
      "Time 99 with reward: 606.6887265259545\n",
      "Running experiment 5 for ete\n",
      "Time 50 with reward: 504.8318040456984\n",
      "Time 51 with reward: 518.3474199246674\n",
      "Time 52 with reward: 631.1589793470954\n",
      "Time 53 with reward: 546.6056327340528\n",
      "Time 54 with reward: 581.1650873395972\n",
      "Time 55 with reward: 537.9853808345867\n",
      "Time 56 with reward: 608.1514330489949\n",
      "Time 57 with reward: 466.52198749263033\n",
      "Time 58 with reward: 423.36491414158286\n",
      "Time 59 with reward: 700.0472838102521\n",
      "Time 60 with reward: 578.2052973857786\n",
      "Time 61 with reward: 647.294246438928\n",
      "Time 62 with reward: 513.2043878157276\n",
      "Time 63 with reward: 417.93312905113885\n",
      "Time 64 with reward: 530.7182151269739\n",
      "Time 65 with reward: 644.2173965840409\n",
      "Time 66 with reward: 532.2764320391989\n",
      "Time 67 with reward: 539.4226133300667\n",
      "Time 68 with reward: 555.9065423748151\n",
      "Time 69 with reward: 749.3361276909732\n",
      "Time 70 with reward: 485.9419215500698\n",
      "Time 71 with reward: 621.2992986685447\n",
      "Time 72 with reward: 717.5427827425976\n",
      "Time 73 with reward: 537.7059953156925\n",
      "Time 74 with reward: 558.2653178030018\n",
      "Time 75 with reward: 601.3110135913441\n",
      "Time 76 with reward: 625.761854391505\n",
      "Time 77 with reward: 686.3985595013776\n",
      "Time 78 with reward: 610.7204599212142\n",
      "Time 79 with reward: 646.0793894006413\n",
      "Time 80 with reward: 544.215502627042\n",
      "Time 81 with reward: 376.39370280086814\n",
      "Time 82 with reward: 463.41269931683985\n",
      "Time 83 with reward: 455.1101519084349\n",
      "Time 84 with reward: 510.19739296714806\n",
      "Time 85 with reward: 441.79498230655827\n",
      "Time 86 with reward: 527.6543915720099\n",
      "Time 87 with reward: 612.1122277958677\n",
      "Time 88 with reward: 569.9310648785312\n",
      "Time 89 with reward: 510.8140763534002\n",
      "Time 90 with reward: 583.3169748158944\n",
      "Time 91 with reward: 456.33297664004994\n",
      "Time 92 with reward: 482.593280229419\n",
      "Time 93 with reward: 515.2196220146595\n",
      "Time 94 with reward: 608.0411391283691\n",
      "Time 95 with reward: 510.470716479462\n",
      "Time 96 with reward: 599.5880098559131\n",
      "Time 97 with reward: 437.8969723739069\n",
      "Time 98 with reward: 536.4656642498453\n",
      "Time 99 with reward: 616.8611333310653\n",
      "Running experiment 6 for ete\n",
      "Time 50 with reward: 511.8694486474584\n",
      "Time 51 with reward: 506.2235719203519\n",
      "Time 52 with reward: 627.9356710479029\n",
      "Time 53 with reward: 542.2955974619781\n",
      "Time 54 with reward: 576.7025316166369\n",
      "Time 55 with reward: 532.218955201789\n",
      "Time 56 with reward: 617.4523426897572\n",
      "Time 57 with reward: 458.24444733607686\n",
      "Time 58 with reward: 407.2742093838142\n",
      "Time 59 with reward: 711.363609005263\n",
      "Time 60 with reward: 589.6643548223185\n",
      "Time 61 with reward: 657.5551993398806\n",
      "Time 62 with reward: 525.4481365947088\n",
      "Time 63 with reward: 428.746370236553\n",
      "Time 64 with reward: 517.5783974428463\n",
      "Time 65 with reward: 636.1883540432664\n",
      "Time 66 with reward: 514.017791276296\n",
      "Time 67 with reward: 543.8851690530271\n",
      "Time 68 with reward: 555.5947185350176\n",
      "Time 69 with reward: 754.670180578282\n",
      "Time 70 with reward: 495.45594010643674\n",
      "Time 71 with reward: 611.4700703820554\n",
      "Time 72 with reward: 728.9230822926523\n",
      "Time 73 with reward: 515.5963681595823\n",
      "Time 74 with reward: 536.5041645773289\n",
      "Time 75 with reward: 585.8439565131705\n",
      "Time 76 with reward: 621.4831737982543\n",
      "Time 77 with reward: 686.3985595013776\n",
      "Time 78 with reward: 612.1762021509118\n",
      "Time 79 with reward: 622.0611095498975\n",
      "Time 80 with reward: 540.0007963888353\n",
      "Time 81 with reward: 370.16358100841273\n",
      "Time 82 with reward: 428.3467643094692\n",
      "Time 83 with reward: 459.4528068567296\n",
      "Time 84 with reward: 521.2018943223613\n",
      "Time 85 with reward: 442.29068127606547\n",
      "Time 86 with reward: 526.1672946634884\n",
      "Time 87 with reward: 606.6582741338932\n",
      "Time 88 with reward: 556.9111479690694\n",
      "Time 89 with reward: 498.601682253243\n",
      "Time 90 with reward: 564.5903384582607\n",
      "Time 91 with reward: 438.91659925560066\n",
      "Time 92 with reward: 476.7201101082441\n",
      "Time 93 with reward: 522.7216109071028\n",
      "Time 94 with reward: 604.109803352359\n",
      "Time 95 with reward: 491.51365509378127\n",
      "Time 96 with reward: 600.9238513109449\n",
      "Time 97 with reward: 440.59660930816233\n",
      "Time 98 with reward: 552.4284202975261\n",
      "Time 99 with reward: 616.1175848768046\n",
      "Running experiment 7 for ete\n",
      "Time 50 with reward: 510.5022525136027\n",
      "Time 51 with reward: 510.03790822291944\n",
      "Time 52 with reward: 613.3007085198317\n",
      "Time 53 with reward: 541.6473780415854\n",
      "Time 54 with reward: 566.6167761501841\n",
      "Time 55 with reward: 529.9876929726363\n",
      "Time 56 with reward: 612.5826340931313\n",
      "Time 57 with reward: 468.7212781765885\n",
      "Time 58 with reward: 420.7898252627832\n",
      "Time 59 with reward: 689.1381114889068\n",
      "Time 60 with reward: 582.8105853502681\n",
      "Time 61 with reward: 642.2965891322143\n",
      "Time 62 with reward: 496.47158368025976\n",
      "Time 63 with reward: 423.4417741268303\n",
      "Time 64 with reward: 506.2934269266595\n",
      "Time 65 with reward: 616.2234666864294\n",
      "Time 66 with reward: 534.4757227231571\n",
      "Time 67 with reward: 550.7635102658753\n",
      "Time 68 with reward: 541.4554549764536\n",
      "Time 69 with reward: 755.878073323226\n",
      "Time 70 with reward: 505.3205387218626\n",
      "Time 71 with reward: 600.5966207930719\n",
      "Time 72 with reward: 720.1178716213973\n",
      "Time 73 with reward: 515.009370570627\n",
      "Time 74 with reward: 556.6816268632165\n",
      "Time 75 with reward: 578.1826642318155\n",
      "Time 76 with reward: 586.9574670967411\n",
      "Time 77 with reward: 666.5248731198653\n",
      "Time 78 with reward: 609.1054143026049\n",
      "Time 79 with reward: 644.8401419768733\n",
      "Time 80 with reward: 522.2064999166156\n",
      "Time 81 with reward: 379.369161615307\n",
      "Time 82 with reward: 430.9582204447257\n",
      "Time 83 with reward: 453.3112311601159\n",
      "Time 84 with reward: 500.12031519590516\n",
      "Time 85 with reward: 433.81296330522815\n",
      "Time 86 with reward: 501.3232900039739\n",
      "Time 87 with reward: 573.6428605467626\n",
      "Time 88 with reward: 562.8934202767712\n",
      "Time 89 with reward: 489.7870485287857\n",
      "Time 90 with reward: 592.8017595863662\n",
      "Time 91 with reward: 500.19673727858617\n",
      "Time 92 with reward: 480.3300151904169\n",
      "Time 93 with reward: 504.6670213034449\n",
      "Time 94 with reward: 611.622590832941\n",
      "Time 95 with reward: 488.33376505894756\n",
      "Time 96 with reward: 598.7804870466084\n",
      "Time 97 with reward: 448.3458308189888\n",
      "Time 98 with reward: 533.1470269167849\n",
      "Time 99 with reward: 602.5379946427917\n",
      "Running experiment 8 for ete\n",
      "Time 50 with reward: 495.33897133980435\n",
      "Time 51 with reward: 389.86081901637783\n",
      "Time 52 with reward: 576.422810622325\n",
      "Time 53 with reward: 533.9221114051863\n",
      "Time 54 with reward: 575.6145396463587\n",
      "Time 55 with reward: 525.9248590542896\n",
      "Time 56 with reward: 592.7089477116191\n",
      "Time 57 with reward: 447.97671149709805\n",
      "Time 58 with reward: 405.317851883166\n",
      "Time 59 with reward: 670.7993051440387\n",
      "Time 60 with reward: 570.9345868307062\n",
      "Time 61 with reward: 637.6175386033244\n",
      "Time 62 with reward: 526.7513583735206\n",
      "Time 63 with reward: 418.00538722671354\n",
      "Time 64 with reward: 458.46858119802573\n",
      "Time 65 with reward: 629.0541154102426\n",
      "Time 66 with reward: 513.1462941119473\n",
      "Time 67 with reward: 545.9251563480735\n",
      "Time 68 with reward: 537.4885982230003\n",
      "Time 69 with reward: 737.2122796866577\n",
      "Time 70 with reward: 483.2105462689099\n",
      "Time 71 with reward: 599.3462223777399\n",
      "Time 72 with reward: 673.9497189981817\n",
      "Time 73 with reward: 501.86387741353406\n",
      "Time 74 with reward: 525.3717145120278\n",
      "Time 75 with reward: 587.9792728420848\n",
      "Time 76 with reward: 619.1559344042082\n",
      "Time 77 with reward: 661.1268458775123\n",
      "Time 78 with reward: 583.8337006787398\n",
      "Time 79 with reward: 635.2274084963136\n",
      "Time 80 with reward: 518.1756688081184\n",
      "Time 81 with reward: 369.876328909413\n",
      "Time 82 with reward: 436.3242761418842\n",
      "Time 83 with reward: 420.8843446222818\n",
      "Time 84 with reward: 503.34362349509763\n",
      "Time 85 with reward: 436.84563049426833\n",
      "Time 86 with reward: 488.0807122169266\n",
      "Time 87 with reward: 588.1763556270345\n",
      "Time 88 with reward: 553.528536280965\n",
      "Time 89 with reward: 489.11122200980583\n",
      "Time 90 with reward: 568.6493926116032\n",
      "Time 91 with reward: 484.63370303551903\n",
      "Time 92 with reward: 468.1108381522335\n",
      "Time 93 with reward: 508.0540287028116\n",
      "Time 94 with reward: 601.5141583828741\n",
      "Time 95 with reward: 471.3972754782338\n",
      "Time 96 with reward: 580.6703363201715\n",
      "Time 97 with reward: 436.13343671883166\n",
      "Time 98 with reward: 525.406795393628\n",
      "Time 99 with reward: 598.9075334699338\n",
      "Running experiment 9 for ete\n",
      "Time 50 with reward: 513.3891652321998\n",
      "Time 51 with reward: 509.85403309320986\n",
      "Time 52 with reward: 636.3091571046947\n",
      "Time 53 with reward: 543.1670946263267\n",
      "Time 54 with reward: 588.2027319413573\n",
      "Time 55 with reward: 533.3703046607407\n",
      "Time 56 with reward: 616.7727685905404\n",
      "Time 57 with reward: 459.64364627978216\n",
      "Time 58 with reward: 416.39124389486676\n",
      "Time 59 with reward: 716.6976618925719\n",
      "Time 60 with reward: 607.2434214860045\n",
      "Time 61 with reward: 664.0971449721335\n",
      "Time 62 with reward: 527.7753759887548\n",
      "Time 63 with reward: 435.67978292426756\n",
      "Time 64 with reward: 500.21142987382746\n",
      "Time 65 with reward: 618.4635355195704\n",
      "Time 66 with reward: 524.3427185322925\n",
      "Time 67 with reward: 556.9690603175329\n",
      "Time 68 with reward: 533.110572945769\n",
      "Time 69 with reward: 762.7958171503203\n",
      "Time 70 with reward: 508.8310991200548\n",
      "Time 71 with reward: 629.4808616602047\n",
      "Time 72 with reward: 726.9716410934477\n",
      "Time 73 with reward: 537.4581458309389\n",
      "Time 74 with reward: 544.0375081485961\n",
      "Time 75 with reward: 601.4389623014321\n",
      "Time 76 with reward: 621.4831737982543\n",
      "Time 77 with reward: 687.4865514716556\n",
      "Time 78 with reward: 607.7136464279513\n",
      "Time 79 with reward: 659.2912293752348\n",
      "Time 80 with reward: 545.9830686965371\n",
      "Time 81 with reward: 393.13262697902957\n",
      "Time 82 with reward: 456.50300342516766\n",
      "Time 83 with reward: 460.908549086427\n",
      "Time 84 with reward: 526.2880977249167\n",
      "Time 85 with reward: 453.5618326106311\n",
      "Time 86 with reward: 515.1193750439164\n",
      "Time 87 with reward: 615.8952094196113\n",
      "Time 88 with reward: 572.5701281123747\n",
      "Time 89 with reward: 504.5199802059009\n",
      "Time 90 with reward: 577.5185776379021\n",
      "Time 91 with reward: 500.324685988674\n",
      "Time 92 with reward: 495.1174981693737\n",
      "Time 93 with reward: 519.1864787681127\n",
      "Time 94 with reward: 629.1690378204071\n",
      "Time 95 with reward: 508.45530092521335\n",
      "Time 96 with reward: 620.237864367906\n",
      "Time 97 with reward: 446.2105144900745\n",
      "Time 98 with reward: 554.7556596915722\n",
      "Time 99 with reward: 615.933709747095\n",
      "Running experiment 10 for ete\n",
      "Time 50 with reward: 503.31208746095695\n",
      "Time 51 with reward: 513.197242167068\n",
      "Time 52 with reward: 630.8471555072979\n",
      "Time 53 with reward: 544.6541915348482\n",
      "Time 54 with reward: 576.4220624556634\n",
      "Time 55 with reward: 533.4588507565825\n",
      "Time 56 with reward: 603.5609286159468\n",
      "Time 57 with reward: 467.4820307528206\n",
      "Time 58 with reward: 415.27189724576454\n",
      "Time 59 with reward: 689.633810458414\n",
      "Time 60 with reward: 605.9075800309728\n",
      "Time 61 with reward: 641.4644945821119\n",
      "Time 62 with reward: 502.26259581816237\n",
      "Time 63 with reward: 419.4454605957908\n",
      "Time 64 with reward: 507.81314351140094\n",
      "Time 65 with reward: 625.6671080108758\n",
      "Time 66 with reward: 513.953816921252\n",
      "Time 67 with reward: 556.5932621226915\n",
      "Time 68 with reward: 540.0636871018\n",
      "Time 69 with reward: 749.6479515307706\n",
      "Time 70 with reward: 526.1856230423891\n",
      "Time 71 with reward: 613.7333354210575\n",
      "Time 72 with reward: 717.5427827425976\n",
      "Time 73 with reward: 534.1395084978785\n",
      "Time 74 with reward: 561.6725012319039\n",
      "Time 75 with reward: 583.140918924283\n",
      "Time 76 with reward: 619.1559344042082\n",
      "Time 77 with reward: 664.9738018562998\n",
      "Time 78 with reward: 613.9437682204067\n",
      "Time 79 with reward: 643.5682748768855\n",
      "Time 80 with reward: 539.9368220337914\n",
      "Time 81 with reward: 382.06415126877243\n",
      "Time 82 with reward: 462.02093144218634\n",
      "Time 83 with reward: 455.07879722961087\n",
      "Time 84 with reward: 501.1443328111394\n",
      "Time 85 with reward: 435.33267988996954\n",
      "Time 86 with reward: 514.4121938461124\n",
      "Time 87 with reward: 595.7096991983017\n",
      "Time 88 with reward: 564.1652873767589\n",
      "Time 89 with reward: 500.7369985821573\n",
      "Time 90 with reward: 570.7287825208957\n",
      "Time 91 with reward: 485.4092542996293\n",
      "Time 92 with reward: 477.5070768268636\n",
      "Time 93 with reward: 508.64632170358266\n",
      "Time 94 with reward: 611.2861952523457\n",
      "Time 95 with reward: 497.1389757302026\n",
      "Time 96 with reward: 606.5303254238053\n",
      "Time 97 with reward: 445.3390173257259\n",
      "Time 98 with reward: 535.7221157955846\n",
      "Time 99 with reward: 612.5824527378146\n"
     ]
    }
   ],
   "source": [
    "Y_t_exp_times = []\n",
    "# Run the experiment 10 times\n",
    "for times in range(10):\n",
    "    print(f\"Running experiment {times + 1} for ete\")\n",
    "\n",
    "    Y_t_exp = exploration_then_exploitation(50)\n",
    "\n",
    "    # Append the results of this experiment\n",
    "    Y_t_exp_times.append(Y_t_exp)\n",
    "    \n",
    "\n",
    "# Convert lists to NumPy arrays for easier computation\n",
    "Y_t_exp_times = np.array(Y_t_exp_times)\n",
    "# Save the results to .npy files\n",
    "np.save(f'Y_t_exp_time-ete-under-interference-n20-k9-d55-s10_10times.npy', Y_t_exp_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34e9f70-91f6-4e3a-99c7-0ce66c584bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
