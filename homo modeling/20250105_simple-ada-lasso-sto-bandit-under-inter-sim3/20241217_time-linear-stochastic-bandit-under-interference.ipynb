{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9902420-c692-4b92-8f14-cc9e38f5ad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.spatial import distance\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "import numpy as np\n",
    "from scipy.optimize import differential_evolution\n",
    "from scipy.stats import rankdata\n",
    "from sklearn.cross_decomposition import CCA\n",
    "import math\n",
    "import copy\n",
    "from math import log\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "066e083a-5463-48c4-85ff-9f72c3f34c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: [3.0143364  4.21106128 4.49056789 6.84433191 4.38055757 4.59035439\n",
      " 5.62650526 4.348927   3.26627828 2.23006741]\n"
     ]
    }
   ],
   "source": [
    "with open('relation_matrix_dic.pkl', 'rb') as f:\n",
    "    relation_matrix_dic = pickle.load(f)\n",
    "\n",
    "# define a new adjacency_matrix_dic: adjacency_matrix_dic[i] = relation_matrix_dic[i].iloc[0:20, 0:20]\n",
    "adjacency_matrix_dic = {i: relation_matrix_dic[i].iloc[0:20, 0:20] for i in relation_matrix_dic}\n",
    "\n",
    "# Copy adjacency_matrix_dic and extend its keys\n",
    "adjacency_matrix_dic_new = adjacency_matrix_dic.copy()\n",
    "\n",
    "for i in range(100):\n",
    "    adjacency_matrix_dic_new[i + 100] = adjacency_matrix_dic[i]\n",
    "\n",
    "# Update the original adjacency_matrix_dic with the extended dictionary\n",
    "adjacency_matrix_dic = adjacency_matrix_dic_new\n",
    "\n",
    "# Read beta back from the file\n",
    "beta = np.load('beta.npy')\n",
    "print(\"beta:\", beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d332fd2c-fa0a-423e-85b3-3696d82b78e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(adjacency_matrix_dic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdf4173d-6c1e-48b5-8a95-c4c812402b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# follow the beta, only 1,2,3 related is non-zero\n",
    "K = 14\n",
    "\n",
    "# Compute total length\n",
    "total_length = 1 + K + K + math.comb(K, 2)\n",
    "total_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "122ff145-5cc5-4027-884e-8ca6906e5242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_beta: [3.0143364  4.21106128 4.49056789 6.84433191 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "new_beta = np.zeros(total_length)\n",
    "\n",
    "# Assign values based on the provided instructions\n",
    "new_beta[0:4] = beta[0:4]            # Copy the first four elements unchanged\n",
    "new_beta[10:13] = beta[4:7]          # Copy beta[4:6] into new_beta[11:13]\n",
    "new_beta[20:22] = beta[7:9]          # Copy beta[7:8] into new_beta[20:21]\n",
    "\n",
    "# Print the new_beta array\n",
    "print(\"new_beta:\", new_beta)\n",
    "\n",
    "beta = new_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "246a72a4-4af8-400a-8999-31402191f36e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position 0: beta = 3.0143363957683738\n",
      "Position 1: beta = 4.211061281023643\n",
      "Position 2: beta = 4.490567890019025\n",
      "Position 3: beta = 6.84433191283132\n",
      "Position 4: beta = 0.0\n",
      "Position 5: beta = 0.0\n",
      "Position 6: beta = 0.0\n",
      "Position 7: beta = 0.0\n",
      "Position 8: beta = 0.0\n",
      "Position 9: beta = 0.0\n",
      "Position (1,1): beta = 0.0\n",
      "Position (1,2): beta = 0.0\n",
      "Position (1,3): beta = 0.0\n",
      "Position (1,4): beta = 0.0\n",
      "Position (1,5): beta = 0.0\n",
      "Position (1,6): beta = 0.0\n",
      "Position (1,7): beta = 0.0\n",
      "Position (1,8): beta = 0.0\n",
      "Position (1,9): beta = 0.0\n",
      "Position (2,2): beta = 0.0\n",
      "Position (2,3): beta = 0.0\n",
      "Position (2,4): beta = 0.0\n",
      "Position (2,5): beta = 0.0\n",
      "Position (2,6): beta = 0.0\n",
      "Position (2,7): beta = 0.0\n",
      "Position (2,8): beta = 0.0\n",
      "Position (2,9): beta = 0.0\n",
      "Position (3,3): beta = 0.0\n",
      "Position (3,4): beta = 0.0\n",
      "Position (3,5): beta = 0.0\n",
      "Position (3,6): beta = 0.0\n",
      "Position (3,7): beta = 0.0\n",
      "Position (3,8): beta = 0.0\n",
      "Position (3,9): beta = 0.0\n",
      "Position (4,4): beta = 0.0\n",
      "Position (4,5): beta = 0.0\n",
      "Position (4,6): beta = 0.0\n",
      "Position (4,7): beta = 0.0\n",
      "Position (4,8): beta = 0.0\n",
      "Position (4,9): beta = 0.0\n",
      "Position (5,5): beta = 0.0\n",
      "Position (5,6): beta = 0.0\n",
      "Position (5,7): beta = 0.0\n",
      "Position (5,8): beta = 0.0\n",
      "Position (5,9): beta = 0.0\n",
      "Position (6,6): beta = 0.0\n",
      "Position (6,7): beta = 0.0\n",
      "Position (6,8): beta = 0.0\n",
      "Position (6,9): beta = 0.0\n",
      "Position (7,7): beta = 0.0\n",
      "Position (7,8): beta = 0.0\n",
      "Position (7,9): beta = 0.0\n",
      "Position (8,8): beta = 0.0\n",
      "Position (8,9): beta = 0.0\n",
      "Position (9,9): beta = 0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"Position {i}: beta = {beta[i]}\")\n",
    "\n",
    "idx = 10\n",
    "\n",
    "for i in range(1, 10):\n",
    "    for j in range(1, 10):\n",
    "        if i <= j:\n",
    "            print(f\"Position ({i},{j}): beta = {beta[idx]}\")\n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83e845e5-ddca-4ecc-b0c2-d73301f07fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function named \"counting_treatment\"\n",
    "def counting_treatment(adjacency_matrix, A, K):\n",
    "\n",
    "    # adjacency_matrix is a adjacency matrix dataframe with length 20 * 20 known\n",
    "    # A is a np.array with length 20, each element 1,2,...,K known corresponding to each node in the network\n",
    "\n",
    "    # a empty df named counting_treatment with column named: 0,1,...,K, (1,1), ..., (1,K), (2,2), ..., (2,K),...(K,K)\n",
    "\n",
    "    # add different numbers of different nodes and different edges. For the column 0, directly give 1.\n",
    "\n",
    "    # Define column names\n",
    "    basic_columns = list(range(0, K + 1))  # Including column 0\n",
    "    edge_columns = [(i, j) for i in range(1, K + 1) for j in range(i, K + 1)]\n",
    "    columns = basic_columns + edge_columns\n",
    "    \n",
    "    # Create an empty DataFrame\n",
    "    counting_treatment_df = pd.DataFrame(columns=columns)\n",
    "    counting_treatment_df.loc[0] = 0  # Initialize all counts to 0\n",
    "    \n",
    "    # Column 0 gets the value of 1 as per the description\n",
    "    counting_treatment_df.at[0, 0] = 1\n",
    "\n",
    "    # Count number of nodes with each treatment\n",
    "    for treatment in range(1, K + 1):\n",
    "        counting_treatment_df.at[0, treatment] = sum(A == treatment)\n",
    "    \n",
    "    # Count the number of edges connecting nodes of different treatments\n",
    "    for i in range(len(adjacency_matrix_dic[0])):\n",
    "        for j in range(i + 1, len(adjacency_matrix_dic[0])):  # Iterate only for j > i to avoid double-counting\n",
    "            if adjacency_matrix.iloc[i, j] != 0:  # If there's an edge between node i and node j\n",
    "                treatment_i = A[i]\n",
    "                treatment_j = A[j]\n",
    "                # Use sorted tuple to represent undirected edge\n",
    "                edge = tuple(sorted((treatment_i, treatment_j)))\n",
    "                if edge in counting_treatment_df.columns:\n",
    "                    counting_treatment_df.at[0, edge] += 1\n",
    "                \n",
    "    return counting_treatment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9820eed2-924b-499f-9091-60571f5e32bf",
   "metadata": {},
   "source": [
    "# generate_outcome "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "095cf8e3-4b66-4d19-8554-349b2635b530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_outcome(adjacency_matrix, A, K, beta, error):\n",
    "    # Generate the treatment matrix using the counting_treatment function\n",
    "    counting_treatment_df = counting_treatment(adjacency_matrix, A, K)\n",
    "\n",
    "    # Convert the treatment matrix to a NumPy array for easier calculation\n",
    "    counting_treatment_array = counting_treatment_df.values.flatten()\n",
    "\n",
    "    # Ensure that beta and counting_treatment_array have the same length\n",
    "    if len(beta) != len(counting_treatment_array):\n",
    "        raise ValueError(\"Length of beta does not match the number of columns in counting_treatment.\")\n",
    "\n",
    "    # Compute the outcome Y\n",
    "    Y_exp = np.dot(counting_treatment_array, beta)\n",
    "    \n",
    "    Y = np.dot(counting_treatment_array, beta) + error\n",
    "\n",
    "    # Return the results\n",
    "    return Y, Y_exp, counting_treatment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaa410b-ce4e-44a5-9392-2d3ef9552ad7",
   "metadata": {},
   "source": [
    "# ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa8d339e-f80c-4d8f-a141-553d0b6d850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_linear_model(experimented_data, lambda_value):\n",
    "    # Step 1: Prepare Data (X, Y)\n",
    "    # X consists of all data except 'Y' from experimented_data\n",
    "    X = experimented_data.iloc[:, :-1].values  # Take all columns except the last one (assumed 'Y' is the last column)\n",
    "    Y = experimented_data['Y'].values.reshape(-1, 1)  # Rewards (Y) as a column vector\n",
    "    \n",
    "    # Step 2: Fit Lasso Regression Model with lambda (alpha) as the regularization parameter\n",
    "    ridge_model = Ridge(alpha=lambda_value)  # Lambda corresponds to the alpha parameter in Lasso\n",
    "    \n",
    "    # Train the Lasso model\n",
    "    ridge_model.fit(X, Y.ravel())  # Use ravel to convert Y to a 1D array\n",
    "\n",
    "\n",
    "    # Step 4: Compute empirical covariance matrix with regularized\n",
    "    empirical_cov_matrix = (1 / X.shape[0]) * np.dot(X.T, X)\n",
    "    \n",
    "    return ridge_model, empirical_cov_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aae4aad6-0638-4e6a-93df-63906bd992df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_linear_reward(A, model, empirical_cov_matrix, adjacency_matrix, K):\n",
    "\n",
    "    # Predict for new target treatments\n",
    "    # Generate new target treatments using counting_treatment function\n",
    "    counting_treatment_df = counting_treatment(adjacency_matrix, A, K)  # Get DataFrame representation\n",
    "    \n",
    "    # Convert the dataframe to a NumPy array for GPy compatibility\n",
    "    X_target = counting_treatment_df.values  # Convert the DataFrame to a NumPy array\n",
    "\n",
    "    # Calculate the weighted norm before reshaping (for 1D vectors)\n",
    "    if X_target.ndim == 1:\n",
    "        # Direct calculation for 1D vector\n",
    "        norm_2 = np.sqrt(np.dot(X_target, np.dot(np.linalg.pinv(empirical_cov_matrix), X_target)))\n",
    "    else:\n",
    "        # If already 2D, calculate as before\n",
    "        norm_2 = np.sqrt(np.dot(X_target, np.dot(np.linalg.pinv(empirical_cov_matrix), X_target.T))[0, 0])\n",
    "    \n",
    "    # Ensure X_target has the correct shape (num_samples, num_features)\n",
    "    if X_target.ndim == 1:\n",
    "        X_target = X_target.reshape(1, -1)  # Reshape to be a 2D array with one row\n",
    "\n",
    "    # Predict the rewards and the full covariance matrix for the new target treatments\n",
    "    predicted_reward = model.predict(X_target)\n",
    "\n",
    "    # Simplify the results to a scalar value if necessary\n",
    "    # For Lasso, the output of .predict() should be a 1D array, so we can just return the first value\n",
    "    \n",
    "    return predicted_reward[0], norm_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad74a980-c50c-4f5d-9c05-a35965e98ec6",
   "metadata": {},
   "source": [
    "# get the parents for next generation by ucb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22a4d16f-449a-4a30-b7ab-e6f0f60179e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization genetic algorithm\n",
    "def get_children_elite(parent_treatment_dic, parent_reward_predict, model, empirical_cov_matrix, adjacency_matrix, K, generation_num, gama):\n",
    "    child_treatment_dic = {}\n",
    "    child_reward_predict = {}\n",
    "    \n",
    "    # Step 2: Generate children from parent treatments using roulette wheel selection\n",
    "    for i in range(generation_num):\n",
    "        # Use roulette wheel selection to choose two parents based on UCB values\n",
    "        A1, A2 = roulette_wheel_selection(parent_reward_predict, parent_treatment_dic)\n",
    "        \n",
    "        # Crossover: with 0.5 probability choose genes from A1 or A2\n",
    "        child_treatment_dic[i + generation_num] = np.where(np.random.rand(len(A1)) < 0.5, A1, A2)\n",
    "        \n",
    "        # Predict reward and UCB for the new child\n",
    "        predicted_rewards, norm_2 = predict_linear_reward(child_treatment_dic[i + generation_num], model, empirical_cov_matrix, adjacency_matrix, K)\n",
    "        child_reward_predict[i + generation_num] = predicted_rewards + gama * norm_2\n",
    "\n",
    "    # Step 3: Select the top treatments from both parents and children\n",
    "    combined_predict = {**parent_reward_predict, **child_reward_predict}\n",
    "    \n",
    "    # Sort the combined dictionary by UCB values and select the top N treatments\n",
    "    sorted_indices = sorted(combined_predict, key=combined_predict.get, reverse=True)[:generation_num]\n",
    "    child_elite = {idx: combined_predict[idx] for idx in sorted_indices}\n",
    "    \n",
    "    child_elite_treatment_dic = {}\n",
    "    for idx in sorted_indices:\n",
    "        if idx in parent_reward_predict:\n",
    "            child_elite_treatment_dic[idx] = parent_treatment_dic[idx]\n",
    "        else:\n",
    "            child_elite_treatment_dic[idx] = child_treatment_dic[idx]\n",
    "\n",
    "    return child_elite_treatment_dic, child_elite\n",
    "\n",
    "\n",
    "def roulette_wheel_selection(reward_predict, treatment_dic):\n",
    "\n",
    "    values = np.array(list(reward_predict.values()))\n",
    "\n",
    "    # Ensure no negative values by shifting if necessary\n",
    "    if np.min(values) < 0:\n",
    "        values = values - np.min(values)  # Shift values to make them non-negative\n",
    "    \n",
    "    total = np.sum(values)\n",
    "    \n",
    "    if total > 0:\n",
    "        probabilities = values / total  # Normalize to create valid probabilities\n",
    "    else:\n",
    "        # If all UCB values are zero, use uniform probability\n",
    "        probabilities = np.ones_like(values) / len(values)  # Equal probability if all UCB are zero\n",
    "\n",
    "    # Select two parents based on the computed probabilities\n",
    "    selected_indices = np.random.choice(len(values), size=2, p=probabilities, replace=False)\n",
    "    return treatment_dic[selected_indices[0]], treatment_dic[selected_indices[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e06823c-3da0-499c-8323-e96a59b92516",
   "metadata": {},
   "source": [
    "# get the next experiment by ucb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8dde1c17-7766-411a-9de3-e193e878cc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the next experiment\n",
    "\n",
    "def get_next_experiment(model, empirical_cov_matrix, adjacency_matrix, K, generation_num, epoch, gama):\n",
    "    parent_treatment_dic = {i: np.random.randint(1, K+1, size=adjacency_matrix.shape[0]) for i in range(100)}\n",
    "    parent_reward_predict = {}\n",
    "    \n",
    "    # Step 1: Calculate UCB for parent treatments\n",
    "    for i in range(generation_num):\n",
    "        predicted_rewards, norm_2 = predict_linear_reward(parent_treatment_dic[i], model, empirical_cov_matrix, adjacency_matrix, K)\n",
    "        # UCB\n",
    "        parent_reward_predict[i] = predicted_rewards + gama * norm_2\n",
    "\n",
    "    \n",
    "    # Epoch loop\n",
    "    # Initialize variables\n",
    "    no_progress_counter = 0  # Counter to track epochs without improvement\n",
    "    best = -np.inf  # To store the best UCB value so far\n",
    "    \n",
    "    \n",
    "    # Epoch loop\n",
    "    for k in range(1, epoch + 1):\n",
    "        # Get the elite children treatments and UCBs\n",
    "        child_elite_treatment_dic, child_elite = get_children_elite(parent_treatment_dic, parent_reward_predict, model, empirical_cov_matrix, adjacency_matrix, K, generation_num, gama)\n",
    "        \n",
    "        # Reassign parent treatments and UCB predictions with renumbered indices\n",
    "        parent_treatment_dic = {new_idx: treatment for new_idx, treatment in enumerate(child_elite_treatment_dic.values())}\n",
    "        parent_reward_predict = {new_idx: sur for new_idx, sur in enumerate(child_elite.values())}\n",
    "    \n",
    "        # Get the best UCB in this epoch\n",
    "        current_best = parent_reward_predict[0]\n",
    "    \n",
    "        # Print the UCB of the best treatment after each epoch\n",
    "        # print(f\"Epoch {k}: Best UCB = {current_best}\")\n",
    "    \n",
    "        # Check if the best UCB has improved\n",
    "        if current_best > best:\n",
    "            best = current_best  # Update the best UCB\n",
    "            no_progress_counter = 0  # Reset the counter\n",
    "        else:\n",
    "            no_progress_counter += 1  # No improvement, increment the counter\n",
    "    \n",
    "        # If no improvement for 10 epochs, break the loop\n",
    "        if no_progress_counter >= 10:\n",
    "            # print(f\"No improvement for 10 consecutive epochs. Stopping at epoch {k}.\")\n",
    "            break\n",
    "\n",
    "    return parent_treatment_dic[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4aef02-bccc-4efc-9e9e-fda44d9f3dad",
   "metadata": {},
   "source": [
    "# initial experiment 0: for a selected vector treatments, we get rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c19dd89d-1b39-4067-b7ed-cef98fc09f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward_array(lambda_value):\n",
    "\n",
    "    generation_num = 100\n",
    "    epoch = 100\n",
    "    delta = 0.1\n",
    "\n",
    "    T = 200\n",
    "    a = 2\n",
    "    d = len(beta)\n",
    "    sigma = 1\n",
    "    L = 1\n",
    "    \n",
    "    # Get the length of adjacency_matrix_dic (number of keys)\n",
    "    length = len(adjacency_matrix_dic)\n",
    "    \n",
    "    # Generate the error array from a normal distribution with the specified length\n",
    "    mean = 0  # Define the mean\n",
    "    std_dev = 2\n",
    "    \n",
    "    error_array = mean + std_dev * np.random.randn(length)\n",
    "\n",
    "\n",
    "    # Initialize empty lists to store A and reward over time\n",
    "    A_time = []\n",
    "    reward_time = []\n",
    "    \n",
    "    # Create an array A , with length matching the adjacency matrix\n",
    "    A = np.random.randint(1, K + 1, size=adjacency_matrix_dic[0].shape[0])\n",
    "    \n",
    "    Y, Y_exp, counting_treatment_df = generate_outcome(adjacency_matrix_dic[0], A, K, beta, error_array[0])\n",
    "    \n",
    "    # get experimented_data\n",
    "    # Create a DataFrame for Y\n",
    "    Y_df = pd.DataFrame([Y], columns=['Y'])\n",
    "    # Concatenate treatment and Y_df to form experimented_data\n",
    "    experimented_data = pd.concat([counting_treatment_df, Y_df], axis=1)\n",
    "    \n",
    "    A_time.append(A)\n",
    "    reward_time.append(Y_exp)\n",
    "\n",
    "    print(f\"Time {0} with reward: {Y_exp}\")\n",
    "\n",
    "    for t in range(1,T):  # Iterate for T time steps\n",
    "\n",
    "        model, empirical_cov_matrix = predict_linear_model(experimented_data, lambda_value)\n",
    "    \n",
    "        # Calculate beta\n",
    "        gama = 2 * np.sqrt(2*(np.log(1/delta) + 0.5 * len(beta) * np.log(1 + t * (L**2) / (len(beta) * lambda_value)))) + np.sqrt(lambda_value)*10\n",
    "        \n",
    "        # Get the next treatment vector A\n",
    "        A = get_next_experiment(model, empirical_cov_matrix, adjacency_matrix_dic[t], K, generation_num, epoch, gama)\n",
    "        \n",
    "        # Generate the outcome based on A\n",
    "        Y, Y_exp, counting_treatment_df = generate_outcome(adjacency_matrix_dic[t], A, K, beta, error_array[t])\n",
    "        \n",
    "        # Create a DataFrame for Y\n",
    "        Y_df = pd.DataFrame([Y], columns=['Y'])\n",
    "        \n",
    "        # Concatenate transformed_treatment and Y_df to form experimented_data\n",
    "        experimented_data = pd.concat([experimented_data, pd.concat([counting_treatment_df, Y_df], axis=1)], ignore_index=True)\n",
    "    \n",
    "        # Append current treatment and reward to their respective lists\n",
    "        A_time.append(A)\n",
    "        reward_time.append(Y_exp)\n",
    "    \n",
    "        # Print the reward for the current time step\n",
    "        print(f\"Time {t} with reward: {Y}\")\n",
    "        # print(f\"Time {t} with treatment: {A}\")\n",
    "\n",
    "    # Convert reward_time to a NumPy array\n",
    "    reward_time = np.array(reward_time)\n",
    "\n",
    "    return reward_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0e4c1105-8d1f-4961-b153-20838e3bdcb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment 1 for linear\n",
      "Time 0 with reward: 39.093293218136324\n",
      "Time 1 with reward: 33.02874033357154\n",
      "Time 2 with reward: 22.29794665356426\n",
      "Time 3 with reward: 27.502607012348815\n",
      "Time 4 with reward: 124.90160034078656\n",
      "Time 5 with reward: 18.290927273604048\n",
      "Time 6 with reward: 114.55606369240566\n",
      "Time 7 with reward: 111.46292997300971\n",
      "Time 8 with reward: 118.35887743846176\n",
      "Time 9 with reward: 92.54286094770407\n",
      "Time 10 with reward: 72.0363991246371\n",
      "Time 11 with reward: 24.094497190855115\n",
      "Time 12 with reward: 26.668357181719617\n",
      "Time 13 with reward: 72.61216769269257\n",
      "Time 14 with reward: 2.728227155696125\n",
      "Time 15 with reward: 44.7855904744967\n",
      "Time 16 with reward: 1.4908591521334207\n",
      "Time 17 with reward: 56.13434945542694\n",
      "Time 18 with reward: 11.342756241443347\n",
      "Time 19 with reward: 48.41282857424339\n",
      "Time 20 with reward: 24.424940786531174\n",
      "Time 21 with reward: 29.106994306742\n",
      "Time 22 with reward: 41.97277140616802\n",
      "Time 23 with reward: 22.01188044146327\n",
      "Time 24 with reward: 94.99374728254783\n",
      "Time 25 with reward: 41.95380288592097\n",
      "Time 26 with reward: 54.43397525146758\n",
      "Time 27 with reward: 43.38018951613146\n",
      "Time 28 with reward: 56.72194362585934\n",
      "Time 29 with reward: 9.703570794824206\n",
      "Time 30 with reward: 11.447115323794849\n",
      "Time 31 with reward: 26.79103492309804\n",
      "Time 32 with reward: 9.115151366475633\n",
      "Time 33 with reward: 7.67155238017372\n",
      "Time 34 with reward: 15.921535261827435\n",
      "Time 35 with reward: 51.9684542519737\n",
      "Time 36 with reward: 75.23825254465424\n",
      "Time 37 with reward: 64.32447012491079\n",
      "Time 38 with reward: 0.2632948246616147\n",
      "Time 39 with reward: 8.44689103857811\n",
      "Time 40 with reward: 5.210276885080675\n",
      "Time 41 with reward: 2.1426044575032255\n",
      "Time 42 with reward: 0.368030925084764\n",
      "Time 43 with reward: 44.321418735348125\n",
      "Time 44 with reward: 89.75008206350432\n",
      "Time 45 with reward: 1.5850759011694024\n",
      "Time 46 with reward: 54.42202124220596\n",
      "Time 47 with reward: 35.963521746938646\n",
      "Time 48 with reward: 4.792614573863884\n",
      "Time 49 with reward: -0.9759814998323724\n",
      "Time 50 with reward: 5.511453631630841\n",
      "Time 51 with reward: 14.273452637384446\n",
      "Time 52 with reward: 4.027761124829777\n",
      "Time 53 with reward: 28.644348292337\n",
      "Time 54 with reward: 1.4079000008330629\n",
      "Time 55 with reward: 57.76166245461566\n",
      "Time 56 with reward: 3.6173460462008085\n",
      "Time 57 with reward: 3.6582774252694477\n",
      "Time 58 with reward: 2.6596411630793755\n",
      "Time 59 with reward: 9.204789000129383\n",
      "Time 60 with reward: 5.732365123293821\n",
      "Time 61 with reward: 10.191219037046595\n",
      "Time 62 with reward: 1.7799782798963701\n",
      "Time 63 with reward: 29.803340539530343\n",
      "Time 64 with reward: 49.342026476728286\n",
      "Time 65 with reward: 8.249584756200791\n",
      "Time 66 with reward: 18.867908548500914\n",
      "Time 67 with reward: 29.756213254837558\n",
      "Time 68 with reward: 3.340715596721432\n",
      "Time 69 with reward: 5.4199415800989375\n",
      "Time 70 with reward: 86.56240480691739\n",
      "Time 71 with reward: 2.5924257903121264\n",
      "Time 72 with reward: 51.15098355471372\n",
      "Time 73 with reward: 4.942718640635572\n",
      "Time 74 with reward: 41.23714764354149\n",
      "Time 75 with reward: 10.65066559723794\n",
      "Time 76 with reward: 21.899183007240776\n",
      "Time 77 with reward: 39.923276878838095\n",
      "Time 78 with reward: 15.789934268338579\n",
      "Time 79 with reward: 38.799687166005334\n",
      "Time 80 with reward: 53.16481888320205\n",
      "Time 81 with reward: 35.17530796170997\n",
      "Time 82 with reward: 61.935980090624504\n",
      "Time 83 with reward: 9.058644438998936\n",
      "Time 84 with reward: 9.380314701124377\n",
      "Time 85 with reward: -2.012689378829207\n",
      "Time 86 with reward: 39.88096671824925\n",
      "Time 87 with reward: -0.38730763769892995\n",
      "Time 88 with reward: 7.209914551833019\n",
      "Time 89 with reward: 56.01673508275153\n",
      "Time 90 with reward: 56.02737093402367\n",
      "Time 91 with reward: 34.57285012483898\n",
      "Time 92 with reward: 33.92701036087235\n",
      "Time 93 with reward: 1.2408426641729078\n",
      "Time 94 with reward: 39.86974502326561\n",
      "Time 95 with reward: 11.794096271222125\n",
      "Time 96 with reward: 2.695856598680954\n",
      "Time 97 with reward: 18.352424005769745\n",
      "Time 98 with reward: 11.048512796806605\n",
      "Time 99 with reward: -0.1391849569689727\n",
      "Time 100 with reward: 8.824855667210963\n",
      "Time 101 with reward: 18.82259963836902\n",
      "Time 102 with reward: 3.888858989791719\n",
      "Time 103 with reward: 30.737837112888634\n",
      "Time 104 with reward: 4.012942455145563\n",
      "Time 105 with reward: 11.391742341282031\n",
      "Time 106 with reward: 4.585780245410491\n",
      "Time 107 with reward: 5.867925103829973\n",
      "Time 108 with reward: 4.544262829856504\n",
      "Time 109 with reward: 48.93717463780344\n",
      "Time 110 with reward: 4.081874986208435\n",
      "Time 111 with reward: 4.161545375779749\n",
      "Time 112 with reward: 10.034912182773049\n",
      "Time 113 with reward: 5.441546145445124\n",
      "Time 114 with reward: 88.2515770569328\n",
      "Time 115 with reward: 14.501060701295868\n",
      "Time 116 with reward: 5.257152462565073\n",
      "Time 117 with reward: 13.130665413912432\n",
      "Time 118 with reward: 1.6849162667337843\n",
      "Time 119 with reward: 9.049476955653812\n",
      "Time 120 with reward: 40.53060728604265\n",
      "Time 121 with reward: 45.474348577612574\n",
      "Time 122 with reward: 58.51193364181067\n",
      "Time 123 with reward: 14.533220791972967\n",
      "Time 124 with reward: 23.839126419068656\n",
      "Time 125 with reward: 1.0633866556043654\n",
      "Time 126 with reward: 72.23124322441511\n",
      "Time 127 with reward: 13.265389626319585\n",
      "Time 128 with reward: 3.459130932751757\n",
      "Time 129 with reward: 11.362543567312047\n",
      "Time 130 with reward: 9.608491732547519\n",
      "Time 131 with reward: 28.734835624445125\n",
      "Time 132 with reward: 34.87157240945435\n",
      "Time 133 with reward: 20.392024448525195\n",
      "Time 134 with reward: 92.30165244780095\n",
      "Time 135 with reward: 58.89521221217326\n",
      "Time 136 with reward: 77.54116645934477\n",
      "Time 137 with reward: 66.05318232614638\n",
      "Time 138 with reward: 96.42611867905002\n",
      "Time 139 with reward: 13.06348791524393\n",
      "Time 140 with reward: 27.692314471513015\n",
      "Time 141 with reward: 58.91183740561941\n",
      "Time 142 with reward: 7.37285430493378\n",
      "Time 143 with reward: 116.9351862679567\n",
      "Time 144 with reward: 33.091417677874226\n",
      "Time 145 with reward: 3.337100324098403\n",
      "Time 146 with reward: 14.424267177135889\n",
      "Time 147 with reward: 10.332487250605963\n",
      "Time 148 with reward: 57.84220421582955\n",
      "Time 149 with reward: 66.05268151677637\n",
      "Time 150 with reward: 67.77290508851199\n",
      "Time 151 with reward: 41.64637943495455\n",
      "Time 152 with reward: 47.46099250847249\n",
      "Time 153 with reward: 79.00747953064185\n",
      "Time 154 with reward: 2.595716747357595\n",
      "Time 155 with reward: 49.45679462984342\n",
      "Time 156 with reward: 54.210639973609865\n",
      "Time 157 with reward: 36.76819228642105\n",
      "Time 158 with reward: -0.5520045765436303\n",
      "Time 159 with reward: 21.34383038219242\n",
      "Time 160 with reward: 24.38180020519536\n",
      "Time 161 with reward: 52.082042502271975\n",
      "Time 162 with reward: 13.54709311954912\n",
      "Time 163 with reward: 84.27921466897885\n",
      "Time 164 with reward: 74.19444775414425\n",
      "Time 165 with reward: 54.598658182668316\n",
      "Time 166 with reward: 67.99810752814142\n",
      "Time 167 with reward: 31.758886406309536\n",
      "Time 168 with reward: 76.35464950640603\n",
      "Time 169 with reward: 40.89919011956678\n",
      "Time 170 with reward: 54.97410130527381\n",
      "Time 171 with reward: 29.017056338057895\n",
      "Time 172 with reward: 0.4877886612068778\n",
      "Time 173 with reward: 63.36990532915557\n",
      "Time 174 with reward: 11.572647935759825\n",
      "Time 175 with reward: 61.06196539433667\n",
      "Time 176 with reward: 82.71926419915069\n",
      "Time 177 with reward: 4.644749671748109\n",
      "Time 178 with reward: 8.350189120792765\n",
      "Time 179 with reward: 3.8232151488534845\n",
      "Time 180 with reward: 77.57151957146978\n",
      "Time 181 with reward: 15.626409105522649\n",
      "Time 182 with reward: 63.697667234256976\n",
      "Time 183 with reward: 63.02211294935543\n",
      "Time 184 with reward: 36.561701034431934\n",
      "Time 185 with reward: 56.32301529159163\n",
      "Time 186 with reward: 111.45849447829154\n",
      "Time 187 with reward: 6.207008267825468\n",
      "Time 188 with reward: 58.68373420073996\n",
      "Time 189 with reward: 116.47695490671578\n",
      "Time 190 with reward: 90.15791385672748\n",
      "Time 191 with reward: 1.810453670941511\n",
      "Time 192 with reward: 89.18343825529523\n",
      "Time 193 with reward: 4.701988114979063\n",
      "Time 194 with reward: 9.614311300644257\n",
      "Time 195 with reward: 115.11513063332661\n",
      "Time 196 with reward: 12.416062014715996\n",
      "Time 197 with reward: 35.73649038569898\n",
      "Time 198 with reward: 6.218874330067053\n",
      "Time 199 with reward: 9.264702012977105\n",
      "Running experiment 2 for linear\n",
      "Time 0 with reward: 18.28079087064698\n",
      "Time 1 with reward: 38.105220983131886\n",
      "Time 2 with reward: 34.79338349077484\n",
      "Time 3 with reward: 72.93377713829683\n",
      "Time 4 with reward: 3.6299345489491612\n",
      "Time 5 with reward: 43.62464456565534\n",
      "Time 6 with reward: 24.59731967409924\n",
      "Time 7 with reward: 36.684665816721\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m times \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning experiment \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimes\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for linear\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m     Y_t_exp \u001b[38;5;241m=\u001b[39m get_reward_array(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Append the results of this experiment\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     Y_t_exp_times\u001b[38;5;241m.\u001b[39mappend(Y_t_exp)\n",
      "Cell \u001b[1;32mIn[34], line 51\u001b[0m, in \u001b[0;36mget_reward_array\u001b[1;34m(lambda_value)\u001b[0m\n\u001b[0;32m     48\u001b[0m gama \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m(np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mdelta) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(beta) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m t \u001b[38;5;241m*\u001b[39m (L\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mlen\u001b[39m(beta) \u001b[38;5;241m*\u001b[39m lambda_value)))) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(lambda_value)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Get the next treatment vector A\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m A \u001b[38;5;241m=\u001b[39m get_next_experiment(model, empirical_cov_matrix, adjacency_matrix_dic[t], K, generation_num, epoch, gama)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Generate the outcome based on A\u001b[39;00m\n\u001b[0;32m     54\u001b[0m Y, Y_exp, counting_treatment_df \u001b[38;5;241m=\u001b[39m generate_outcome(adjacency_matrix_dic[t], A, K, beta, error_array[t])\n",
      "Cell \u001b[1;32mIn[31], line 23\u001b[0m, in \u001b[0;36mget_next_experiment\u001b[1;34m(model, empirical_cov_matrix, adjacency_matrix, K, generation_num, epoch, gama)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Epoch loop\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Get the elite children treatments and UCBs\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     child_elite_treatment_dic, child_elite \u001b[38;5;241m=\u001b[39m get_children_elite(parent_treatment_dic, parent_reward_predict, model, empirical_cov_matrix, adjacency_matrix, K, generation_num, gama)\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Reassign parent treatments and UCB predictions with renumbered indices\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     parent_treatment_dic \u001b[38;5;241m=\u001b[39m {new_idx: treatment \u001b[38;5;28;01mfor\u001b[39;00m new_idx, treatment \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(child_elite_treatment_dic\u001b[38;5;241m.\u001b[39mvalues())}\n",
      "Cell \u001b[1;32mIn[28], line 15\u001b[0m, in \u001b[0;36mget_children_elite\u001b[1;34m(parent_treatment_dic, parent_reward_predict, model, empirical_cov_matrix, adjacency_matrix, K, generation_num, gama)\u001b[0m\n\u001b[0;32m     12\u001b[0m     child_treatment_dic[i \u001b[38;5;241m+\u001b[39m generation_num] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;28mlen\u001b[39m(A1)) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.5\u001b[39m, A1, A2)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# Predict reward and UCB for the new child\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     predicted_rewards, norm_2 \u001b[38;5;241m=\u001b[39m predict_linear_reward(child_treatment_dic[i \u001b[38;5;241m+\u001b[39m generation_num], model, empirical_cov_matrix, adjacency_matrix, K)\n\u001b[0;32m     16\u001b[0m     child_reward_predict[i \u001b[38;5;241m+\u001b[39m generation_num] \u001b[38;5;241m=\u001b[39m predicted_rewards \u001b[38;5;241m+\u001b[39m gama \u001b[38;5;241m*\u001b[39m norm_2\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Step 3: Select the top treatments from both parents and children\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[25], line 16\u001b[0m, in \u001b[0;36mpredict_linear_reward\u001b[1;34m(A, model, empirical_cov_matrix, adjacency_matrix, K)\u001b[0m\n\u001b[0;32m     13\u001b[0m     norm_2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39mdot(X_target, np\u001b[38;5;241m.\u001b[39mdot(np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mpinv(empirical_cov_matrix), X_target)))\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# If already 2D, calculate as before\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     norm_2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39mdot(X_target, np\u001b[38;5;241m.\u001b[39mdot(np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mpinv(empirical_cov_matrix), X_target\u001b[38;5;241m.\u001b[39mT))[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Ensure X_target has the correct shape (num_samples, num_features)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_target\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\linalg\\linalg.py:2022\u001b[0m, in \u001b[0;36mpinv\u001b[1;34m(a, rcond, hermitian)\u001b[0m\n\u001b[0;32m   2020\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrap(res)\n\u001b[0;32m   2021\u001b[0m a \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mconjugate()\n\u001b[1;32m-> 2022\u001b[0m u, s, vt \u001b[38;5;241m=\u001b[39m svd(a, full_matrices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, hermitian\u001b[38;5;241m=\u001b[39mhermitian)\n\u001b[0;32m   2024\u001b[0m \u001b[38;5;66;03m# discard small singular values\u001b[39;00m\n\u001b[0;32m   2025\u001b[0m cutoff \u001b[38;5;241m=\u001b[39m rcond[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, newaxis] \u001b[38;5;241m*\u001b[39m amax(s, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\linalg\\linalg.py:1681\u001b[0m, in \u001b[0;36msvd\u001b[1;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[0;32m   1678\u001b[0m         gufunc \u001b[38;5;241m=\u001b[39m _umath_linalg\u001b[38;5;241m.\u001b[39msvd_n_s\n\u001b[0;32m   1680\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->DdD\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->ddd\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m-> 1681\u001b[0m u, s, vh \u001b[38;5;241m=\u001b[39m gufunc(a, signature\u001b[38;5;241m=\u001b[39msignature, extobj\u001b[38;5;241m=\u001b[39mextobj)\n\u001b[0;32m   1682\u001b[0m u \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1683\u001b[0m s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mastype(_realType(result_t), copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Y_t_exp_times = []\n",
    "# Run the experiment 10 times\n",
    "for times in range(10):\n",
    "    print(f\"Running experiment {times + 1} for linear\")\n",
    "\n",
    "    Y_t_exp = get_reward_array(2)\n",
    "\n",
    "    # Append the results of this experiment\n",
    "    Y_t_exp_times.append(Y_t_exp)\n",
    "    \n",
    "\n",
    "# Convert lists to NumPy arrays for easier computation\n",
    "Y_t_exp_times = np.array(Y_t_exp_times)\n",
    "# Save the results to .npy files\n",
    "np.save(f'Y_t_exp_time-linear-stochastic-bandit-under-interference-n20-k14-d120-s10_10times.npy', Y_t_exp_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5377ac-181e-477c-b3b9-b540c1dd7f74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
